{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSF ETL process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log01042021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log01062021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log02042021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log02062021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log03042021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log14052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log15052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log16052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log17052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log18052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log19052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log20032021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log20052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log21052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log22052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log23052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log24052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log25052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log26052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log27052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log28052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log29052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log30052021.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\Log31052021.csv']\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "Transformation OK, the output file is ready to use\n",
      "['\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log1.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log10.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log11.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log12.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log13.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log14.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log15.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log16.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log17.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log18.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log19.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log2.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log20.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log21.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log22.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log23.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log24.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log3.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log5.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log6.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log7.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log8.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log9.csv']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv' does not exist: b'\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5f20648ab414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# Concatenate all data into one DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv' does not exist: b'\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv'"
     ]
    }
   ],
   "source": [
    "#step 1 Compress log information by time\n",
    "\n",
    "#import necesary libraries\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path = r'\\\\192.168.0.7\\sharedfolder\\ftpdata\\logs'\n",
    "\n",
    "filenames = glob.glob(path + \"\\Log*.csv\")\n",
    "print(filenames)\n",
    "number = 0\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    number = number + 1\n",
    "    data1 = pd.read_csv(filename)\n",
    "    #filetering columns\n",
    "    ds_Q = data1[['Time', 'Pressure[Bar]', 'DP[Bar]', 'Temperature[C]',\n",
    "           'Velocity[m/s]', 'Quality',\n",
    "           'WaterCut[%]', 'Xl', 'WWC[%]', 'ch1[nA]', 'ch2[nA]', 'ch3[nA]',\n",
    "           'ch4[nA]', 'ch5[nA]','TotWLR[%]', 'TotGOR', 'GVF[%]', 'XLM',\n",
    "           'Frg_dp_out', 'GasDen[kg/m3]', 'GasVisc[cP]', 'Kappa',\n",
    "           'LiqDen[kg/m3]', 'LiqVisc[cP]','Qg[m3/s]', 'Ql[m3/s]', 'Qo[m3/s]',\n",
    "           'Qw[m3/s]', 'QgStd[m3/s]', 'QlStd[m3/s]', 'QoStd[m3/s]', 'QwStd[m3/s]',\n",
    "           'TotWLR[%]', 'TotGOR', 'GVF[%]'\n",
    "           ]]\n",
    "\n",
    "    #adding columns with 24hs rates\n",
    "\n",
    "    ds_Q.loc[:,\"Qg[m3/d]\"] = ds_Q.loc[:,\"Qg[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"Ql[m3/d]\"] = ds_Q.loc[:,\"Ql[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"Qo[m3/d]\"] = ds_Q.loc[:,\"Qo[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"Qw[m3/d]\"] = ds_Q.loc[:,\"Qw[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"QgStd[m3/d]\"] = ds_Q.loc[:,\"QgStd[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"QlStd[m3/d]\"] = ds_Q.loc[:,\"QlStd[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"QoStd[m3/d]\"] = ds_Q.loc[:,\"QoStd[m3/s]\"] * 86400\n",
    "    ds_Q.loc[:,\"QwStd[m3/d]\"] = ds_Q.loc[:,\"QwStd[m3/s]\"] * 86400\n",
    "\n",
    "    #cleaning columns\n",
    "\n",
    "    to_drop = ['Qg[m3/s]', 'Ql[m3/s]', 'Qo[m3/s]',\n",
    "           'Qw[m3/s]', 'QgStd[m3/s]', 'QlStd[m3/s]', 'QoStd[m3/s]', 'QwStd[m3/s]',\n",
    "           'TotGOR', 'GVF[%]']\n",
    "\n",
    "    ds_Qf= ds_Q.drop(to_drop, axis=1)\n",
    "\n",
    "    ds = ds_Qf.copy()\n",
    "\n",
    "    ds = ds.set_index('Time')\n",
    "\n",
    "    ds.index = pd.to_datetime(ds.index)\n",
    "\n",
    "    ds2 = ds.groupby(pd.Grouper(freq='60min')).mean() \n",
    "    ds2.head(5)\n",
    "\n",
    "\n",
    "    #export in a way to compare \n",
    "    ds2.to_csv(path+'\\compressed\\log'+str(number)+'.csv')\n",
    "\n",
    "    print(\"Transformation OK, the output file is ready to use\")\n",
    "\n",
    "# get data file names\n",
    "path = r'\\\\192.168.0.7\\sharedfolder\\ftpdata\\logs\\compressed'\n",
    "\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "print(filenames)\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename,low_memory=False).assign(filename = filename))\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_frame = pd.concat(dfs, ignore_index=False,sort=False)\n",
    "\n",
    "big_frame.to_csv(r\"\\\\192.168.0.7\\3tdata\\data_lake\\shell_pad11\\fsf_raw_full.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Final Transformation OK, the output file is ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log1.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log2.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log3.csv', '\\\\\\\\192.168.0.7\\\\sharedfolder\\\\ftpdata\\\\logs\\\\compressed\\\\log4.csv']\n"
     ]
    }
   ],
   "source": [
    "#just TO combine multiple files\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path = r'\\\\192.168.0.7\\sharedfolder\\ftpdata\\logs\\compressed'\n",
    "\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "print(filenames)\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename,low_memory=False).assign(filename = filename))\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_frame = pd.concat(dfs, ignore_index=False,sort=False)\n",
    "\n",
    "big_frame.to_csv(r\"\\\\192.168.0.7\\3tdata\\data_lake\\shell_pad11\\fsf_raw_full.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
